# llama-swap configuration

logLevel: info
sendLoadingState: true

# Default values inherited by all models unless overridden

macros:
  home_dir: "/home/merlin/"
  llama-server: "/home/merlin/git/llama.cpp/build/bin/llama-server"
  llama-server-rocm: "/home/merlin/git/llama.cpp.rom/build/bin/llama-server"
  model-path: "/home/merlin/models/"
  hf-cache: "/home/merlin/.cache/huggingface/hub/"
  extra-storage: "/home/merlin/extra_storage/models/"
  host: "0.0.0.0"
  flash_attn: "on"
  slots: "--slots"
  cache_type_k: "q4_0"
  cache_type_v: "q4_0"
  ngl: 99
  ctx_size: 16384
  temp: 0.7
  # Performance tuning for Strix Halo (shared memory architecture)
  batch_size: 512
  ubatch_size: 512
  parallel: 1
  threads: 14
  cont_batching: "--cont-batching"

models:
  gpt-oss-120b:
    description: "GPT Open Source, 120B version. Useful for programming."
    recommended_use: "Code generation, debugging, programming assistance"
    macros:
      model_path: "${hf-cache}/models--unsloth--gpt-oss-120b-GGUF/snapshots/ff1a82da6ad466e32284fa3d2b86694db3204789/Q4_K_M/gpt-oss-120b-Q4_K_M-00001-of-00002.gguf"
      ctx_size: 32768
      batch_size: 1
      ubatch_size: 1
    cmd: |
      ${llama-server-rocm}
      --port ${PORT}
      --host ${host}
      --flash-attn ${flash_attn}
      ${slots}
      --ctx-size ${ctx_size}
      --cache-type-k ${cache_type_k}
      --cache-type-v ${cache_type_v}
      -ngl ${ngl}
      --model ${model_path}
      --jinja
      ${cont_batching}
      --parallel ${parallel}
      --batch-size ${batch_size}
      --ubatch-size ${ubatch_size}
      --no-mmap
      --threads 16

  gpt-oss-120b-vulkan:
    description: "GPT Open Source, 120B version. Useful for programming."
    recommended_use: "Code generation, debugging, programming assistance"
    macros:
      model_path: "${hf-cache}/models--unsloth--gpt-oss-120b-GGUF/snapshots/ff1a82da6ad466e32284fa3d2b86694db3204789/Q4_K_M/gpt-oss-120b-Q4_K_M-00001-of-00002.gguf"
      ctx_size: 32768
      batch_size: 1
      ubatch_size: 1
    cmd: |
      ${llama-server}
      --port ${PORT}
      --host ${host}
      --flash-attn ${flash_attn}
      ${slots}
      --ctx-size ${ctx_size}
      --cache-type-k ${cache_type_k}
      --cache-type-v ${cache_type_v}
      -ngl ${ngl}
      --model ${model_path}
      --jinja
      ${cont_batching}
      --parallel ${parallel}
      --batch-size ${batch_size}
      --ubatch-size ${ubatch_size}
      --no-mmap
      --threads ${threads}

  Qwen3-Next-80B-A3B-Instruct-UD-Q4-K-XL:
    description: "Next-Generation Qwen3 Model, performs on par with Qwen3-235B models."
    recommended_use: "Code generation, debugging, programming assistance"
    macros:
      model_path: "${extra-storage}/Qwen3-Next-80B-A3B-Instruct-GGUF/Qwen3-Next-80B-A3B-Instruct-UD-Q4_K_XL.gguf"
      ctx_size: 131072
      temp: 0.7
      top-p: 0.8
      top-k: 20
      min-p: 0
      presence-penalty: 1
    cmd: |
      ${llama-server-rocm}
      --port ${PORT}
      --host ${host}
      --flash-attn ${flash_attn}
      ${slots}
      --ctx-size ${ctx_size}
      --temp ${temp}
      --top-p ${top-p}
      --top-k ${top-k}
      --min-p ${min-p}
      # --cache-type-k ${cache_type_k}
      # --cache-type-v ${cache_type_v}
      -ngl ${ngl}
      --model ${model_path}
      ${cont_batching}
      --parallel ${parallel}
      --batch-size ${batch_size}
      --ubatch-size ${ubatch_size}
      --mlock
      --no-mmap
      --threads ${threads}

  Qwen3-Next-80B-A3B-Instruct-Q3-K-S:
    description: "Next-Generation Qwen3 Model, performs on par with Qwen3-235B models."
    recommended_use: "Code generation, debugging, programming assistance"
    macros:
      model_path: "${extra-storage}/Qwen3-Next-80B-A3B-Instruct-GGUF/Qwen3-Next-80B-A3B-Instruct-Q3_K_S.gguf"
      ctx_size: 131072
      temp: 0.7
      top-p: 0.8
      top-k: 20
      min-p: 0
      presence-penalty: 1
    cmd: |
      ${llama-server}
      --port ${PORT}
      --host ${host}
      --flash-attn ${flash_attn}
      ${slots}
      --ctx-size ${ctx_size}
      --temp ${temp}
      --top-p ${top-p}
      --top-k ${top-k}
      --min-p ${min-p}
      # --cache-type-k ${cache_type_k}
      # --cache-type-v ${cache_type_v}
      -ngl ${ngl}
      --model ${model_path}
      ${cont_batching}
      --parallel ${parallel}
      --batch-size ${batch_size}
      --ubatch-size ${ubatch_size}
      --mlock
      --no-mmap
      --threads ${threads}

  Qwen3-Next-80B-A3B-Thinking-UD-Q4-K-XL:
    description: "Next-Generation Qwen3 Model, performs on par with Qwen3-235B models."
    recommended_use: "Code generation, debugging, programming assistance"
    macros:
      model_path: "${extra-storage}/Qwen3-Next-80B-A3B-Thinking-UD-Q4_K_XL.gguf"
      ctx_size: 262144
      temp: 0.7
      top-p: 0.8
      top-k: 20
      min-p: 0
      presence-penalty: 1
    cmd: |
      ${llama-server-rocm}
      --port ${PORT}
      --host ${host}
      --flash-attn ${flash_attn}
      ${slots}
      --ctx-size ${ctx_size}
      --temp ${temp}
      --top-p ${top-p}
      --top-k ${top-k}
      --min-p ${min-p}
      # --cache-type-k ${cache_type_k}
      # --cache-type-v ${cache_type_v}
      -ngl ${ngl}
      --model ${model_path}
      ${cont_batching}
      --parallel ${parallel}
      --batch-size ${batch_size}
      --ubatch-size ${ubatch_size}
      --mlock
      --no-mmap
      --threads ${threads}

  RNJ-1-Instruct:
    description: "Small coding model, good for quick responses"
    recommended_use: "Code generation, debugging, programming assistance"
    macros:
      model_path: "${home_dir}/Downloads/rnj-1-instruct-UD-Q8_K_XL.gguf"
      ctx_size: 16384
      temp: 0.6
      top-p: 0.8
      top-k: 20
      min-p: 0
      presence-penalty: 1
    cmd: |
      ${llama-server}
      --port ${PORT}
      --host ${host}
      --flash-attn ${flash_attn}
      ${slots}
      --ctx-size ${ctx_size}
      --temp ${temp}
      --top-p ${top-p}
      --top-k ${top-k}
      --min-p ${min-p}
      # --cache-type-k ${cache_type_k}
      # --cache-type-v ${cache_type_v}
      -ngl ${ngl}
      --model ${model_path}
      ${cont_batching}
      --parallel ${parallel}
      --batch-size ${batch_size}
      --ubatch-size ${ubatch_size}
      --mlock
      --no-mmap
      --threads ${threads}

  RNJ-1-Instruct-1:
    description: "Small coding model, good for quick responses"
    recommended_use: "Code generation, debugging, programming assistance"
    macros:
      model_path: "${home_dir}/Downloads/rnj-1-instruct-UD-Q8_K_XL.gguf"
      ctx_size: 16384
      temp: 0.6
      top-p: 0.8
      top-k: 20
      min-p: 0
      presence-penalty: 1
    cmd: |
      ${llama-server}
      --port ${PORT}
      --host ${host}
      --flash-attn ${flash_attn}
      ${slots}
      --ctx-size ${ctx_size}
      --temp ${temp}
      --top-p ${top-p}
      --top-k ${top-k}
      --min-p ${min-p}
      # --cache-type-k ${cache_type_k}
      # --cache-type-v ${cache_type_v}
      -ngl ${ngl}
      --model ${model_path}
      ${cont_batching}
      --parallel ${parallel}
      --batch-size ${batch_size}
      --ubatch-size ${ubatch_size}
      --mlock
      --no-mmap
      --threads ${threads}

  RNJ-1-Instruct-2:
    description: "Small coding model, good for quick responses"
    recommended_use: "Code generation, debugging, programming assistance"
    macros:
      model_path: "${home_dir}/Downloads/rnj-1-instruct-UD-Q8_K_XL.gguf"
      ctx_size: 16384
      temp: 0.6
      top-p: 0.8
      top-k: 20
      min-p: 0
      presence-penalty: 1
    cmd: |
      ${llama-server}
      --port ${PORT}
      --host ${host}
      --flash-attn ${flash_attn}
      ${slots}
      --ctx-size ${ctx_size}
      --temp ${temp}
      --top-p ${top-p}
      --top-k ${top-k}
      --min-p ${min-p}
      # --cache-type-k ${cache_type_k}
      # --cache-type-v ${cache_type_v}
      -ngl ${ngl}
      --model ${model_path}
      ${cont_batching}
      --parallel ${parallel}
      --batch-size ${batch_size}
      --ubatch-size ${ubatch_size}
      --mlock
      --no-mmap
      --threads ${threads}

  RNJ-1-Instruct-3:
    description: "Small coding model, good for quick responses"
    recommended_use: "Code generation, debugging, programming assistance"
    macros:
      model_path: "${home_dir}/Downloads/rnj-1-instruct-UD-Q8_K_XL.gguf"
      ctx_size: 16384
      temp: 0.6
      top-p: 0.8
      top-k: 20
      min-p: 0
      presence-penalty: 1
    cmd: |
      ${llama-server}
      --port ${PORT}
      --host ${host}
      --flash-attn ${flash_attn}
      ${slots}
      --ctx-size ${ctx_size}
      --temp ${temp}
      --top-p ${top-p}
      --top-k ${top-k}
      --min-p ${min-p}
      # --cache-type-k ${cache_type_k}
      # --cache-type-v ${cache_type_v}
      -ngl ${ngl}
      --model ${model_path}
      ${cont_batching}
      --parallel ${parallel}
      --batch-size ${batch_size}
      --ubatch-size ${ubatch_size}
      --mlock
      --no-mmap
      --threads ${threads}

groups:
  "gpt-pair-rocm":
    swap: false
    members:
      - "gpt-oss-120b"
      - "RNJ-1-Instruct"
  "gpt-pair-vulkan":
    swap: false
    members:
      - "gpt-oss-120b-vulkan"
      - "RNJ-1-Instruct-1"
  "qwen3-next-instruct":
    swap: false
    members:
      - "Qwen3-Next-80B-A3B-Instruct-UD-Q4-K-XL"
      - "RNJ-1-Instruct-2"
  "qwen3-next-thinking":
    swap: false
    members:
      - "Qwen3-Next-80B-A3B-Thinking-UD-Q4-K-XL"
      - "RNJ-1-Instruct-3"

# Validation rules (optional enhancement)
validation:
  required_fields:
    - model_path
    - cmd
  valid_ctx_sizes:
    [4096, 8192, 16384, 32768, 65536, 131072, 196608, 262144, 327680]
  max_temperature: 2.0
  min_temperature: 0.0
