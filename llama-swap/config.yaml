# llama-swap configuration
# Environment variables can override these values:
# - LLAMA_SERVER_PATH: Path to llama-server binary
# - MODEL_PATH: Base directory for models
# - HF_CACHE: HuggingFace cache directory

logLevel: info
sendLoadingState: true

# Default values inherited by all models unless overridden

macros:
  llama-server: "/home/merlin/git/llama.cpp/build/bin/llama-server"
  model-path: "models/"
  hf-cache: "/home/merlin/.cache/huggingface/hub/"
  host: "0.0.0.0"
  flash_attn: "on"
  slots: "--slots"
  cache_type_k: "q8_0"
  cache_type_v: "q8_0"
  ngl: 99
  ctx_size: 32768
  temp: 0.7

models:
  qwen-coder-32b:
    description: "Qwen2.5 Coder 32B Instruct - Specialized for coding tasks"
    recommended_use: "Code generation, debugging, programming assistance"
    macros:
      model_path: "${hf-cache}/models--Triangle104--Qwen2.5-Coder-32B-Instruct-Q4_K_M-GGUF/snapshots/2246fc23fc869f1be602abaf3b8b5c6b8e23c13d/qwen2.5-coder-32b-instruct-q4_k_m.gguf"
      ctx_size: 32768
    cmd: |
      ${llama-server}
      --port ${PORT}
      --host ${host}
      --flash-attn ${flash_attn}
      ${slots}
      --ctx-size ${ctx_size}
      --cache-type-k ${cache_type_k}
      --cache-type-v ${cache_type_v}
      -ngl ${ngl}
      --model ${model_path}

  qwq-32b:
    description: "Qwen QwQ 32B - Advanced reasoning model"
    recommended_use: "Complex reasoning, problem-solving, analysis"
    macros:
      model_path: "${hf-cache}/models--bartowski--Qwen_QwQ-32B-GGUF/snapshots/390cc7b31baedc55a4d094802995e75f40b4a86d/Qwen_QwQ-32B-Q4_K_M.gguf"
      ctx_size: 65536
      temp: 0.6
    cmd: |
      ${llama-server}
      --port ${PORT}
      --host ${host}
      --flash-attn ${flash_attn}
      --metrics
      ${slots}
      --ctx-size ${ctx_size}
      --cache-type-k ${cache_type_k}
      --cache-type-v ${cache_type_v}
      -ngl ${ngl}
      --model ${model_path}
      --samplers "top_k;top_p;min_p;temperature;dry;typ_p;xtc"
      --temp ${temp}
      --repeat-penalty 1.1
      --dry-multiplier 0.5
      --min-p 0.01
      --top-k 40
      --top-p 0.95

  qwen3-coder-q6:
    description: "Qwen3 Coder 30B Q6_K - Balanced coding model"
    recommended_use: "General coding, script development"
    macros:
      model_path: "${model-path}/qwen-coder/Qwen3-Coder-30B-A3B-Instruct-Q6_K.gguf"
      ctx_size: 327680
    cmd: |
      ${llama-server}
      --port ${PORT}
      --host ${host}
      --flash-attn ${flash_attn}
      ${slots}
      --ctx-size ${ctx_size}
      --model ${model_path}
      --temp ${temp}

  qwen3-coder-q4:
    description: "Qwen3 Coder 30B Q4_K_M - Lightweight coding model"
    recommended_use: "Quick coding tasks, resource-constrained environments"
    macros:
      model_path: "${model-path}/qwen-coder/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf"
      ctx_size: 327680
    cmd: |
      ${llama-server}
      --port ${PORT}
      --host ${host}
      --flash-attn ${flash_attn}
      ${slots}
      --ctx-size ${ctx_size}
      --model ${model_path}
      --temp ${temp}

  qwen3-coder-q6-xl:
    description: "Qwen3 Coder 30B Q6_K_XL - High-quality coding model"
    recommended_use: "Complex coding tasks requiring high accuracy"
    macros:
      model_path: "${model-path}/qwen-coder/Qwen3-Coder-30B-A3B-Instruct-UD-Q6_K_XL.gguf"
      ctx_size: 327680
    cmd: |
      ${llama-server}
      --port ${PORT}
      --host ${host}
      --flash-attn ${flash_attn}
      ${slots}
      --ctx-size ${ctx_size}
      --model ${model_path}
      --temp ${temp}
  starcoder-2-15b-q8:
    description: "StarCoder2 15B Q8 - high quality coding model."
    recommended_use: "Complex coding tasks requiring high accuracy"
    macros:
      model_path: "${hf-cache}/models--dranger003--starcoder2-15b-GGUF/snapshots/652898e2235cd61c5648fefb4e03e66a434f8b4d/ggml-starcoder2-15b-q8_0.gguf"
      ctx_size: 327680
    cmd: |
      ${llama-server}
      --port ${PORT}
      --host ${host}
      --flash-attn ${flash_attn}
      ${slots}
      --ctx-size ${ctx_size}
      --model ${model_path}
      --temp ${temp}

  kokoro:
    description: "Kokoro espeak Q8 - text to speech model"
    recommended_use: "high quality text to speech model"
    macros:
      model_path: "${hf-cache}/models--mmwillet2--Kokoro_GGUF/snapshots/e9e81d8e813948353195c9db77ef065476335c8d/Kokoro_espeak_Q8.gguf"
      ctx_size: 327680
    cmd: |
      ${llama-server}
      --port ${PORT}
      --host ${host}
      --flash-attn ${flash_attn}
      ${slots}
      --ctx-size ${ctx_size}
      --model ${model_path}
      --temp ${temp}

groups:
  architect-and-editor:
    description: "Paired models for complex reasoning and coding"
    swap: false
    exclusive: true
    members:
      - qwq-32b
      - qwen-coder-32b

# Validation rules (optional enhancement)
validation:
  required_fields:
    - model_path
    - cmd
  valid_ctx_sizes: [4096, 8192, 16384, 32768, 65536, 131072, 262144, 327680]
  max_temperature: 2.0
  min_temperature: 0.0
