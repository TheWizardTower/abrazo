{
  "$schema": "https://charm.land/crush.json",
  "providers": {
    "lemonaid": {
      "type": "openai-compat",
      "base_url": "http://turkishDelight:8000/api/v1",
      "api_key": "lemonaid",
      "name": "Lemonaid ROCm enabled server",
      "id": "lemonaid",
      "models": [
        {
          "id": "Qwen3-Coder-30B-A3B-Instruct-GGUF",
          "name": "Qwen3 Coder 30B A3B",
          "context_window": 131072,
          "default_max_tokens": 32768,
          "options": {},
          "can_reason": true,
          "supports_attachments": false,
          "default_reasoning_effort": "high",
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        },
        {
          "id": "gpt-oss-120b-GGUF",
          "name": "GPT-OSS 20B",
          "context_window": 131072,
          "default_max_tokens": 16384,
          "options": {},
          "can_reason": true,
          "supports_attachments": false,
          "default_reasoning_effort": "high",
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        }
      ]
    },
    "llama-swap": {
      "type": "openai-compat",
      "base_url": "http://192.168.1.140:8080/v1",
      "name": "Llama-swap - Llama.cpp",
      "id": "llamacpp",
      "models": [
        {
          "id": "Qwen3-next-80B-A3B-Instruct-UD-Q4-K-XL",
          "name": "Qwen3-Next-80B-A3B-Instruct-UD-Q4_K_XL",
          "context_window": 262144,
          "default_max_tokens": 32768,
          "options": {},
          "can_reason": false,
          "supports_attachments": false,
          "default_reasoning_effort": "high",
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        },
        {
          "id": "RNJ-1-Instruct",
          "name": "RNJ-1-Instruct",
          "context_window": 262144,
          "default_max_tokens": 16384,
          "options": {},
          "can_reason": true,
          "supports_attachments": false,
          "default_reasoning_effort": "high",
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        },
        {
          "id": "gpt-oss-120b",
          "name": "GPT-OSS 120B",
          "context_window": 131072,
          "default_max_tokens": 16384,
          "options": {},
          "can_reason": true,
          "supports_attachments": false,
          "default_reasoning_effort": "high",
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        },
        {
          "id": "starcoder-2-15b-q8",
          "name": "StarCoder-2 15B Q8",
          "context_window": 196608,
          "default_max_tokens": 16384,
          "options": {},
          "can_reason": true,
          "supports_attachments": false,
          "default_reasoning_effort": "high",
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        },
        {
          "id": "qwq-32b",
          "name": "Qwen2.5 Architect",
          "context_window": 131072,
          "default_max_tokens": 51200,
          "options": {},
          "can_reason": true,
          "supports_attachments": false,
          "default_reasoning_effort": "high",
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        },
        {
          "id": "qwen3-coder-q6-xl",
          "name": "Qwen3 Q6 XL model. For heavy tasks.",
          "context_window": 196608,
          "default_max_tokens": 51200,
          "options": {},
          "can_reason": true,
          "supports_attachments": false,
          "default_reasoning_effort": "high",
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        },
        {
          "id": "qwen-coder-32b",
          "name": "Qwen2.5 Coder 32B",
          "context_window": 131072,
          "default_max_tokens": 51200,
          "options": {},
          "can_reason": true,
          "supports_attachments": false,
          "default_reasoning_effort": "high",
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        },
        {
          "id": "qwen3-coder-q6",
          "name": "Qwen3 Coder Q6",
          "context_window": 196608,
          "default_max_tokens": 51200,
          "options": {},
          "can_reason": false,
          "supports_attachments": false,
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        },
        {
          "id": "qwen3-coder-q4",
          "name": "Qwen3 Coder Q4 (Fast)",
          "context_window": 196608,
          "default_max_tokens": 51200,
          "options": {},
          "can_reason": false,
          "supports_attachments": false,
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        }
      ]
    }
  },
  "mcp": {
    "serena": {
      "type": "stdio",
      "command": "serena-mcp-server",
      "args": [
        "--project-from-cwd"
      ]
    },
    "git": {
      "type": "stdio",
      "command": "uvx",
      "args": [
        "mcp-server-git"
      ]
    },
    "sequential-thinking": {
      "type": "stdio",
      "command": "npx",
      "args": [
        "-y",
        "@modelcontextprotocol/server-sequential-thinking"
      ]
    },
    "kubernetes": {
      "type": "stdio",
      "command": "npx",
      "args": [
        "-y",
        "kubernetes-mcp-server@latest"
      ]
    },
    "mcp_k8s": {
      "type": "stdio",
      "command": "mcp-k8s",
      "args": []
    },
    "nixos": {
      "type": "stdio",
      "command": "uvx",
      "args": [
        "mcp-nixos"
      ]
    },
    "mcp-gp-pdf-reader": {
      "type": "stdio",
      "command": "node",
      "args": [
        "/home/merlin/git/mcp_gp_pdf_reader/index.js"
      ]
    }
  },
  "lsp": {
    "rust": {
      "command": "rust-analyzer"
    },
    "python": {
      "command": "pylsp"
    },
    "javascript": {
      "command": "typescript-language-server",
      "args": [
        "--stdio"
      ]
    },
    "typescript": {
      "command": "typescript-language-server",
      "args": [
        "--stdio"
      ]
    },
    "go": {
      "command": "gopls"
    },
    "java": {
      "command": "jdtls"
    },
    "c": {
      "command": "clangd"
    },
    "cpp": {
      "command": "clangd"
    },
    "yaml": {
      "command": "yaml-language-server",
      "args": [
        "--stdio"
      ]
    },
    "json": {
      "command": "json-languageserver",
      "args": [
        "--stdio"
      ]
    }
  },
  "permissions": {
    "allowed_tools": [
      "view",
      "ls",
      "grep",
      "edit",
      "mcp_context7_get-library-doc"
    ]
  },
  "models": {
    "large": {
      "model": "Qwen3-Coder-30B-A3B-Instruct-GGUF",
      "provider": "lemonaid",
      "reasoning_effort": "high",
      "max_tokens": 16384
    }
  },
  "recent_models": {
    "large": [
      {
        "model": "Qwen3-Coder-30B-A3B-Instruct-GGUF",
        "provider": "lemonaid"
      },
      {
        "model": "gpt-oss-120b-GGUF",
        "provider": "lemonaid"
      },
      {
        "model": "qwen3-coder-q4",
        "provider": "llama-swap"
      },
      {
        "model": "qwen3-coder-q6-xl",
        "provider": "llama-swap"
      },
      {
        "model": "gpt-oss-20b",
        "provider": "llama-swap"
      }
    ]
  }
}
