{
  "$schema": "https://charm.land/crush.json",
  "providers": {
    "lemonaid": {
      "type": "openai-compat",
      "base_url": "http://turkishDelight:8000/api/v1",
      "api_key": "lemonaid",
      "name": "Lemonaid ROCm enabled server",
      "id": "lemonaid",
      "models": [
        {
          "id": "Qwen3-Coder-30B-A3B-Instruct-GGUF",
          "name": "Qwen3 Coder 30B A3B",
          "context_window": 131072,
          "default_max_tokens": 16384,
          "options": {},
          "can_reason": true,
          "supports_attachments": false,
          "default_reasoning_effort": "high",
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        },
        {
          "id": "gpt-oss-120b-GGUF",
          "name": "GPT-OSS 20B",
          "context_window": 131072,
          "default_max_tokens": 16384,
          "options": {},
          "can_reason": true,
          "supports_attachments": false,
          "default_reasoning_effort": "high",
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        }
      ]
    },
    "llama-swap": {
      "type": "openai-compat",
      "base_url": "http://turkishDelight:8080/v1",
      "name": "Llama-swap - Llama.cpp",
      "id": "llamacpp",
      "models": [
        {
          "id": "gpt-oss-20b",
          "name": "GPT-OSS 20B",
          "context_window": 131072,
          "default_max_tokens": 16384,
          "options": {},
          "can_reason": true,
          "supports_attachments": false,
          "default_reasoning_effort": "high",
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        },
        {
          "id": "starcoder-2-15b-q8",
          "name": "StarCoder-2 15B Q8",
          "context_window": 196608,
          "default_max_tokens": 16384,
          "options": {},
          "can_reason": true,
          "supports_attachments": false,
          "default_reasoning_effort": "high",
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        },
        {
          "id": "qwq-32b",
          "name": "Qwen2.5 Architect",
          "context_window": 131072,
          "default_max_tokens": 51200,
          "options": {},
          "can_reason": true,
          "supports_attachments": false,
          "default_reasoning_effort": "high",
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        },
        {
          "id": "qwen3-coder-q6-xl",
          "name": "Qwen3 Q6 XL model. For heavy tasks.",
          "context_window": 196608,
          "default_max_tokens": 51200,
          "options": {},
          "can_reason": true,
          "supports_attachments": false,
          "default_reasoning_effort": "high",
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        },
        {
          "id": "qwen-coder-32b",
          "name": "Qwen2.5 Coder 32B",
          "context_window": 131072,
          "default_max_tokens": 51200,
          "options": {},
          "can_reason": true,
          "supports_attachments": false,
          "default_reasoning_effort": "high",
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        },
        {
          "id": "qwen3-coder-q6",
          "name": "Qwen3 Coder Q6",
          "context_window": 196608,
          "default_max_tokens": 51200,
          "options": {},
          "can_reason": false,
          "supports_attachments": false,
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        },
        {
          "id": "qwen3-coder-q4",
          "name": "Qwen3 Coder Q4 (Fast)",
          "context_window": 196608,
          "default_max_tokens": 51200,
          "options": {},
          "can_reason": false,
          "supports_attachments": false,
          "cost_per_1m_in": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_out_cached": 0
        }
      ]
    }
  },
  "mcp": {
    "serena-stdio": {
      "type": "stdio",
      "command": "serena",
      "args": [
        "start-mcp-server",
        "--project-from-cwd"
      ]
    },
    "sourcerer": {
      "type": "stdio",
      "command": "uvx",
      "args": [
        "sourcerer-mcp"
      ]
    },
    "semgrep": {
      "type": "stdio",
      "command": "npx",
      "args": [
        "-y",
        "@modelcontextprotocol/server-semgrep"
      ]
    },
    "filesystem": {
      "type": "stdio",
      "command": "npx",
      "args": [
        "-y",
        "@modelcontextprotocol/server-filesystem",
        "/home/merlin/git"
      ]
    },
    "git": {
      "type": "stdio",
      "command": "uvx",
      "args": [
        "mcp-server-git",
        "--repository",
        "path/to/git/repo"
      ]
    },
    "sequential-thinking": {
      "type": "stdio",
      "command": "npx",
      "args": [
        "-y",
        "@modelcontextprotocol/server-sequential-thinking"
      ]
    }
  },
  "lsp": {
    "rust": {
      "command": "rust-analyzer"
    }
  },
  "models": {
    "large": {
      "model": "gpt-oss-20b",
      "provider": "llama-swap",
      "max_tokens": 51200
    }
  },
  "recent_models": {
    "large": [
      {
        "model": "gpt-oss-20b",
        "provider": "llama-swap"
      }
    ]
  }
}
